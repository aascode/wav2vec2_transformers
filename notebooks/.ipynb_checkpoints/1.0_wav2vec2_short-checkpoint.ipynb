{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSCRIBING AUDIO FILES WITH WAV2VEC2\n",
    "\n",
    "Transcribing audio files into text is among the most tedious and time consuming tasks in business and research. As such I was particularly keen to try [Hugging Face's implementation of Facebook's Wav2Vec2 model](https://huggingface.co/transformers/model_doc/wav2vec2.html) when it was released for transformers 4.3.\n",
    "\n",
    "I was curious to see how well it would perform for short and long speeches, different accents and different \"delivery formats\" - be it formal speeches or a poetry recital. The three notebooks in this repo cover the results from my trials, ranging from a short audio snippet (62s) to a poetry recital (5minutes 34s) and a 12-minute-plus political speech.\n",
    "\n",
    "The accents in these audio clips involve: White American, African American and Singaporean Chinese.\n",
    "\n",
    "I find the results from Wav2Vec2 to be really impressive, and think this can help open up new ways to \"chain link\" NLP tasks directly from audio to textual analysis.\n",
    "\n",
    "Long audio clips are very memory-intensive, however, and efforts to process audio files longer than 90s tend to crash normal work machines and even Colab Pro notebooks. I have a minor workaround in notebooks2.0/2.1 that are somewhat clumsy, but they get the job done within a reasonable period of time. Will figure out a more efficient way to do this at some point.\n",
    "\n",
    "\n",
    "## REFERENCES\n",
    "\n",
    "- Documentation of [Hugging Face's implementation of Wav2Vec2](https://huggingface.co/transformers/model_doc/wav2vec2.html)\n",
    "\n",
    "- Hosted inference [API on Hugging Face](https://huggingface.co/facebook/wav2vec2-base-960h)\n",
    "\n",
    "- Paper on [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)\n",
    "\n",
    "\n",
    "## RESULTS\n",
    "\n",
    "- The output text files of the two longer trials can be downloaded [here](https://www.dropbox.com/s/zx4bfct1zhl18az/amanda_gorman.txt) and [here](https://www.dropbox.com/s/gu3e6ns4x4tty61/lhl_wef.txt)\n",
    "\n",
    "\n",
    "## REQUIREMENTS\n",
    "\n",
    "- [transformers](https://pypi.org/project/transformers/) >= 4.3\n",
    "- [librosa](https://pypi.org/project/librosa/)\n",
    "- if you want to use your own audio clips, make sure to downsample them to 16kHz as the Wav2Vec2 model used here was pretrained and fine-tuned on 16kHz sampled speech audio. I used [Audacity](https://www.audacityteam.org/) to split up the audio files in this repo.\n",
    "\n",
    "\n",
    "## MODELS\n",
    "\n",
    "- There are several versions of the Wav2Vec2 model on Hugging Face's model hub. I haven't tried them out to see what the difference in output quality is like. Check them out [here](https://huggingface.co/models?search=wav2ve).\n",
    "\n",
    "- This repo uses the [wav2vec2-base-960h](https://huggingface.co/facebook/wav2vec2-base-960h) model throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TRANSCRIBE SHORT AUDIO CLIP\n",
    "\n",
    "This is the easiest way to try out Wav2Vec2. The file included in this notebook is a 62s clip of John F Kennedy's famous inaugural speech in 1961. Feel free to swap out with any audio clip of your choice.\n",
    "\n",
    "This works only for English speeches as far as I know.\n",
    "\n",
    "Audio clips longer than 90s tend to crash the notebook, on local machine or Colab Pro, due to out-of-memory issues. Not an issue if you have some super-duper top range gear at work. But if you have more modest computing resources, see notebooks2.0/2.1 for a workaround for longer speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "b1dL-TWaiL2_"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_7_kYLbkiTBF"
   },
   "outputs": [],
   "source": [
    "#load and tokenizer and pre-trained model\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aZHVYnywjEu0",
    "outputId": "61ace4a1-b603-4bb3-b79e-4a96d089df22"
   },
   "outputs": [],
   "source": [
    "#load audio file from folder of choice\n",
    "file_path = \"../audio/jfk.flac\"\n",
    "\n",
    "speech, rate = librosa.load(file_path,sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EKKsO6TsjToR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.4 s, sys: 4.93 s, total: 42.4 s\n",
      "Wall time: 26.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_values = tokenizer(speech, return_tensors = 'pt').input_values\n",
    "\n",
    "#Store logits (non-normalized predictions)\n",
    "logits = model(input_values).logits\n",
    "\n",
    "#Store predicted id's\n",
    "predicted_ids = torch.argmax(logits, dim =-1)\n",
    "\n",
    "#decode the audio to generate text\n",
    "transcript = tokenizer.decode(predicted_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE 1: \n",
    "I ran this on a 32GB ram iMac from late-2015. The run time would vary depending on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "scP9E_yPrGpq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN THE LONG HISTORY OF THE WORLD ONLY A FEW GENERATION HAVE BEEN GRANTED THE ROLE OF DEFENDING FREEDOM IN ITS OUR O MAXIMUM DANGER I DO NOT SHRINK FROM THIS RESPONSIBILITY I WELCOME IT   I DO NOT BELIEVE THAT ANY OF US WOULD EXCHANGE PLACES WITH ANY OTHER PEOPLE OR ANY OTHER GENERATION THE ENERGY THE FAITH THE DEVOTION WHICH WANGE BRANG TO THIS END OF UP WILL NIT OUR COUNTRY AND ALL WHO SERVE IT AND THE GLOW FROM THAT FIRE AND TRULY LIKE THE WORLD AND SO MY FELLOW AMERICA ASK NOT WHAT YOUR COUNTRY CAN DO FOR YOU ASK WHAT YOU CAN DO FOR YOUR COUNTRY\n"
     ]
    }
   ],
   "source": [
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE 2:\n",
    "\n",
    "There are some obvious errors in the transcript, but nothing a human user can't easily and quickly fix. Overall, the results are really good, in my view.\n",
    "\n",
    "Next, let's see how the model performs on a longer audio clip of a different \"delivery format\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "wav2vec2_transformers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
